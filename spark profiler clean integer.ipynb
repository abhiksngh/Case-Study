{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from StringIO import BytesIO\n",
    "except ImportError:\n",
    "    from io import BytesIO\n",
    "\n",
    "try:\n",
    "    from urllib import quote\n",
    "except ImportError:\n",
    "    from urllib.parse import quote\n",
    "\n",
    "import base64\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "#Known imports\n",
    "import databricks.koalas as ks\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spark_df_profiling.formatters as formatters, spark_df_profiling.templates as templates\n",
    "from matplotlib import pyplot as plt\n",
    "from pkg_resources import resource_filename\n",
    "import six\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.functions import (abs as df_abs, col, count, countDistinct, \n",
    "                                   max as df_max, mean, min as df_min,  \n",
    "                                   sum as df_sum, when,variance, stddev, kurtosis, skewness)\n",
    "\n",
    "###Spark initialization and configuration\n",
    "config = pyspark.SparkConf().setAll([((\"spark.sql.execution.arrow.enabled\", \"true\")),('spark.executor.memory', '8g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g')])\n",
    "spark = SparkSession.builder.config(conf=config).master(\"local[*]\").appName(\"Spark Profiler\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Dummy data\n",
    "def gen_pdf(num_records):\n",
    "    return pd.DataFrame(np.random.rand(num_records, 2), columns=list(\"ab\"))\n",
    "\n",
    "def create_sparkFrame():\n",
    "    pdf = gen_pdf(1000)\n",
    "    return spark.createDataFrame(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_data(dataFrame):\n",
    "    table_stats = {\"n\": a.count()}\n",
    "    variable_stats=pd.DataFrame()\n",
    "    for colum in tqdm(a.columns):\n",
    "        variable_stats=pd.concat([variable_stats,describe_data(a, colum, table_stats[\"n\"]).toPandas()],axis=1)   \n",
    "    corr_reject=0.1\n",
    "    table_stats[\"nvar\"] = len(a.columns)\n",
    "    table_stats[\"total_missing\"] = float(variable_stats.loc[\"n_missing\"].sum()) / (table_stats[\"n\"] * table_stats[\"nvar\"])\n",
    "    memsize = 0\n",
    "    table_stats['memsize'] = formatters.fmt_bytesize(memsize)\n",
    "    table_stats['recordsize'] = formatters.fmt_bytesize(memsize / table_stats['n'])\n",
    "    typdict={1.0:'NUM',2.0:'CAT'}\n",
    "    variable_stats.loc['type']=variable_stats.loc['type'].map(typdict)\n",
    "    table_stats.update({k: 0 for k in (\"NUM\", \"DATE\", \"CONST\", \"CAT\", \"UNIQUE\", \"CORR\")})\n",
    "    table_stats.update(dict(variable_stats.loc['type'].value_counts()))\n",
    "    table_stats['REJECTED'] = table_stats['CONST'] + table_stats['CORR']\n",
    "\n",
    "    description_set=describe_sets(table_stats,variable_stats)\n",
    "    return description_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_sets(table_stats,variable_stats): \n",
    "    freq_dict = {}\n",
    "    #for var in variable_stats:\n",
    "       # if \"value_counts\" not in variable_stats[var]:\n",
    "       #     pass\n",
    "       # elif not(variable_stats[var][\"value_counts\"] is np.nan):\n",
    "       #     freq_dict[var] = variable_stats[var][\"value_counts\"]\n",
    "       # else:\n",
    "       #     pass\n",
    "   # try:\n",
    "        #variable_stats = variable_stats.drop(\"value_counts\")\n",
    "   # except ValueError:\n",
    "       # pass\n",
    "\n",
    "    return {'table': table_stats, 'variables': variable_stats.T, 'freq': freq_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_html(sample, stats_object):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate a HTML report from summary statistics and a given sample\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample: DataFrame containing the sample you want to print\n",
    "    stats_object: Dictionary containing summary statistics. Should be generated with an appropriate describe() function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str, containing profile report in HTML format\n",
    "    \"\"\"\n",
    "\n",
    "    n_obs = stats_object['table']['n']\n",
    "\n",
    "    value_formatters = formatters.value_formatters\n",
    "    row_formatters = formatters.row_formatters\n",
    "\n",
    "    if not isinstance(sample, pd.DataFrame):\n",
    "        raise TypeError(\"sample must be of type pandas.DataFrame\")\n",
    "\n",
    "    if not isinstance(stats_object, dict):\n",
    "        raise TypeError(\"stats_object must be of type dict. Did you generate this using the spark_df_profiling.describe() function?\")\n",
    "\n",
    "    if set(stats_object.keys()) != {'table', 'variables', 'freq'}:\n",
    "        raise TypeError(\"stats_object badly formatted. Did you generate this using the spark_df_profiling-eda.describe() function?\")\n",
    "\n",
    "    def fmt(value, name):\n",
    "        if pd.isnull(value):\n",
    "            return \"\"\n",
    "        if name in value_formatters:\n",
    "            return value_formatters[name](value)\n",
    "        elif isinstance(value, float):\n",
    "            return value_formatters[formatters.DEFAULT_FLOAT_FORMATTER](value)\n",
    "        else:\n",
    "            if sys.version_info.major == 3:\n",
    "                return str(value)\n",
    "            else:\n",
    "                return unicode(value)\n",
    "\n",
    "    def freq_table(freqtable, n, var_table, table_template, row_template, max_number_of_items_in_table):\n",
    "\n",
    "        local_var_table = var_table.copy()\n",
    "        freq_other_prefiltered = freqtable[\"***Other Values***\"]\n",
    "        freq_other_prefiltered_num = freqtable[\"***Other Values Distinct Count***\"]\n",
    "        freqtable = freqtable.drop([\"***Other Values***\", \"***Other Values Distinct Count***\"])\n",
    "\n",
    "        freq_rows_html = u''\n",
    "\n",
    "        freq_other = sum(freqtable[max_number_of_items_in_table:]) + freq_other_prefiltered\n",
    "        freq_missing = var_table[\"n_missing\"]\n",
    "        max_freq = max(freqtable.values[0], freq_other, freq_missing)\n",
    "        try:\n",
    "            min_freq = freqtable.values[max_number_of_items_in_table]\n",
    "        except IndexError:\n",
    "            min_freq = 0\n",
    "\n",
    "        # TODO: Correctly sort missing and other\n",
    "\n",
    "        def format_row(freq, label, extra_class=''):\n",
    "            width = int(freq / float(max_freq) * 99) + 1\n",
    "            if width > 20:\n",
    "                label_in_bar = freq\n",
    "                label_after_bar = \"\"\n",
    "            else:\n",
    "                label_in_bar = \"&nbsp;\"\n",
    "                label_after_bar = freq\n",
    "\n",
    "            return row_template.render(label=label,\n",
    "                                       width=width,\n",
    "                                       count=freq,\n",
    "                                       percentage='{:2.1f}'.format(freq / float(n) * 100),\n",
    "                                       extra_class=extra_class,\n",
    "                                       label_in_bar=label_in_bar,\n",
    "                                       label_after_bar=label_after_bar)\n",
    "\n",
    "        for label, freq in six.iteritems(freqtable[0:max_number_of_items_in_table]):\n",
    "            freq_rows_html += format_row(freq, label)\n",
    "\n",
    "        if freq_other > min_freq:\n",
    "            freq_rows_html += format_row(freq_other,\n",
    "                                         \"Other values (%s)\" % (freqtable.count() \n",
    "                                                                + freq_other_prefiltered_num \n",
    "                                                                - max_number_of_items_in_table),\n",
    "                                         extra_class='other')\n",
    "\n",
    "        if freq_missing > min_freq:\n",
    "            freq_rows_html += format_row(freq_missing, \"(Missing)\", extra_class='missing')\n",
    "\n",
    "        return table_template.render(rows=freq_rows_html, varid=hash(idx))\n",
    "\n",
    "    # Variables\n",
    "    rows_html = u\"\"\n",
    "    messages = []\n",
    "\n",
    "    for idx, row in stats_object['variables'].iterrows():\n",
    "\n",
    "        formatted_values = {'varname': idx, 'varid': hash(idx)}\n",
    "        row_classes = {}\n",
    "\n",
    "        for col, value in six.iteritems(row):\n",
    "            formatted_values[col] = fmt(value, col)\n",
    "\n",
    "        for col in set(row.index) & six.viewkeys(row_formatters):\n",
    "            row_classes[col] = row_formatters[col](row[col])\n",
    "            if row_classes[col] == \"alert\" and col in templates.messages:\n",
    "                messages.append(templates.messages[col].format(formatted_values, varname = formatters.fmt_varname(idx)))\n",
    "\n",
    "        if row['type'] == 'CAT':\n",
    "            formatted_values['minifreqtable'] = freq_table(stats_object['freq'][idx], n_obs, stats_object['variables'].ix[idx],\n",
    "                                                           templates.template('mini_freq_table'), templates.template('mini_freq_table_row'), 3)\n",
    "            formatted_values['freqtable'] = freq_table(stats_object['freq'][idx], n_obs, stats_object['variables'].ix[idx],\n",
    "                                                       templates.template('freq_table'), templates.template('freq_table_row'), 20)\n",
    "            if row['distinct_count'] > 50:\n",
    "                messages.append(templates.messages['HIGH_CARDINALITY'].format(formatted_values, varname = formatters.fmt_varname(idx)))\n",
    "                row_classes['distinct_count'] = \"alert\"\n",
    "            else:\n",
    "                row_classes['distinct_count'] = \"\"\n",
    "\n",
    "        if row['type'] == 'UNIQUE':\n",
    "            obs = stats_object['freq'][idx].index\n",
    "\n",
    "            formatted_values['firstn'] = pd.DataFrame(obs[0:3], columns=[\"First 3 values\"]).to_html(classes=\"example_values\", index=False)\n",
    "            formatted_values['lastn'] = pd.DataFrame(obs[-3:], columns=[\"Last 3 values\"]).to_html(classes=\"example_values\", index=False)\n",
    "\n",
    "            if n_obs > 40:\n",
    "                formatted_values['firstn_expanded'] = pd.DataFrame(obs[0:20], index=range(1, 21)).to_html(classes=\"sample table table-hover\", header=False)\n",
    "                formatted_values['lastn_expanded'] = pd.DataFrame(obs[-20:], index=range(n_obs - 20 + 1, n_obs+1)).to_html(classes=\"sample table table-hover\", header=False)\n",
    "            else:\n",
    "                formatted_values['firstn_expanded'] = pd.DataFrame(obs, index=range(1, n_obs+1)).to_html(classes=\"sample table table-hover\", header=False)\n",
    "                formatted_values['lastn_expanded'] = ''\n",
    "\n",
    "        rows_html += templates.row_templates_dict[row['type']].render(values=formatted_values, row_classes=row_classes)\n",
    "\n",
    "        if row['type'] in {'CORR', 'CONST'}:\n",
    "            formatted_values['varname'] = formatters.fmt_varname(idx)\n",
    "            messages.append(templates.messages[row['type']].format(formatted_values))\n",
    "\n",
    "\n",
    "    # Overview\n",
    "    formatted_values = {k: fmt(v, k) for k, v in six.iteritems(stats_object['table'])}\n",
    "\n",
    "    row_classes={}\n",
    "    for col in six.viewkeys(stats_object['table']) & six.viewkeys(row_formatters):\n",
    "        row_classes[col] = row_formatters[col](stats_object['table'][col])\n",
    "        if row_classes[col] == \"alert\" and col in templates.messages:\n",
    "            messages.append(templates.messages[col].format(formatted_values, varname = formatters.fmt_varname(idx)))\n",
    "\n",
    "    messages_html = u''\n",
    "    for msg in messages:\n",
    "        messages_html += templates.message_row.format(message=msg)\n",
    "\n",
    "    overview_html = templates.template('overview').render(values=formatted_values, row_classes = row_classes, messages=messages_html)\n",
    "\n",
    "    # Sample\n",
    "\n",
    "    sample_html = templates.template('sample').render(sample_table_html=sample.to_html(classes=\"sample\"))\n",
    "    # TODO: should be done in the template\n",
    "    return templates.template('base').render({'overview_html': overview_html, 'rows_html': rows_html, 'sample_html': sample_html})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_output(html):\n",
    "    chart = HTML(html)\n",
    "    display(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Index: bigint, Name: string, Score: bigint, Section: string, Score.1: double]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf=pd.read_csv('Book1.csv')\n",
    "a=spark.createDataFrame(pdf)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5380dc29184de9b9c3d970cf8b9835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuabhi15/anaconda3/lib/python3.7/site-packages/pyarrow/__init__.py:157: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream\n",
      "  warnings.warn(\"pyarrow.open_stream is deprecated, please use \"\n"
     ]
    },
    {
     "ename": "ArrowTypeError",
     "evalue": "an integer is required (got type str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-5bf34afafb37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#a=create_sparkFrame()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdescription_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdescription_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhtmlFinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wrapper'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-11584fa1e02b>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(dataFrame)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvariable_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mvariable_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdescribe_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcorr_reject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtable_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nvar\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-3f97cf67c6f8>\u001b[0m in \u001b[0;36mdescribe_data\u001b[0;34m(df, column, nrows)\u001b[0m\n\u001b[1;32m     63\u001b[0m        \u001b[0;31m#        result[\"mode\"] = \"MISSING\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/databricks/koalas/frame.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1661\u001b[0m                     \u001b[0;34m\"'compute.max_rows', this operation is considerably expensive.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m                     .format(max_compute_count))\n\u001b[0;32m-> 1663\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m         \u001b[0;31m# Explode the data to be pairs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/databricks/koalas/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_InternalFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/databricks/koalas/internal.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[0;34m(pdf)\u001b[0m\n\u001b[1;32m    730\u001b[0m         schema = StructType([StructField(str(name), infer_pd_series_spark_type(col),\n\u001b[1;32m    731\u001b[0m                                          nullable=bool(col.isnull().any()))\n\u001b[0;32m--> 732\u001b[0;31m                              for name, col in reset_index.iteritems()])\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreset_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/databricks/koalas/internal.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    730\u001b[0m         schema = StructType([StructField(str(name), infer_pd_series_spark_type(col),\n\u001b[1;32m    731\u001b[0m                                          nullable=bool(col.isnull().any()))\n\u001b[0;32m--> 732\u001b[0;31m                              for name, col in reset_index.iteritems()])\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreset_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/databricks/koalas/typedef.py\u001b[0m in \u001b[0;36minfer_pd_series_spark_type\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty or null dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrow_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetime64_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestampType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Array.from_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: an integer is required (got type str)"
     ]
    }
   ],
   "source": [
    "#a=create_sparkFrame()\n",
    "description_set=process_data(a)\n",
    "sample = a.limit(5).toPandas()\n",
    "html = to_html(sample,description_set)\n",
    "htmlFinal=templates.template('wrapper').render(content=html)\n",
    "custom_output(htmlFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    " def describe_data(df, column, nrows):\n",
    "        column_type = df.select(column).dtypes[0][1]\n",
    "        if (\"array\" in column_type) or (\"stuct\" in column_type) or (\"map\" in column_type):\n",
    "            raise NotImplementedError(\"Column {c} is of type {t} and cannot be analyzed\".format(c=column, t=column_type))\n",
    "        \n",
    "        if column_type in {\"tinyint\", \"smallint\", \"int\", \"bigint\", \"float\", \"double\", \"decimal\"}:\n",
    "            results_data=results_data=ks.DataFrame  (\n",
    "            {'distinct_count': df.select(column).agg(countDistinct(col(column))).collect()[0][0],\n",
    "             'count': df.select(column).na.drop().select(count(col(column))).collect()[0][0],\n",
    "             'mean' : df.select(column).na.drop().agg(mean(col(column))).collect()[0][0],\n",
    "             'min': df.select(column).na.drop().agg(df_min(col(column))).collect()[0][0],\n",
    "             \"max\": df.select(column).na.drop().agg(df_max(col(column))).collect()[0][0],\n",
    "             \"variance\": df.select(column).na.drop().agg(variance(col(column))).collect()[0][0],\n",
    "             \"kurtosis\" : df.select(column).na.drop().agg(kurtosis(col(column))).collect()[0][0],\n",
    "             \"std\"    : df.select(column).na.drop().agg(stddev(col(column))).collect()[0][0],\n",
    "             \"skewness\" : df.select(column).na.drop().agg(skewness(col(column))).collect()[0][0],\n",
    "             \"sum\"     : df.select(column).na.drop().agg(df_sum(col(column))).collect()[0][0],\n",
    "             \"type\" : 1\n",
    "            },index=[column])  \n",
    "        elif column_type in {'string'}:\n",
    "            results_data=results_data=ks.DataFrame  (\n",
    "            {'distinct_count': df.select(column).agg(countDistinct(col(column))).collect()[0][0],\n",
    "             'count': df.select(column).na.drop().select(count(col(column))).collect()[0][0],\n",
    "             'mean' : '',\n",
    "             'min': '',\n",
    "             \"max\": '',\n",
    "             \"variance\": '',\n",
    "             \"kurtosis\" : '',\n",
    "             \"std\"    : '',\n",
    "             \"skewness\" : '',\n",
    "             \"sum\"     : '',\n",
    "             \"type\" : 2\n",
    "            },index=[column])  \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        \n",
    "        results_data[\"p_unique\"] = results_data[\"distinct_count\"] / results_data[\"count\"]\n",
    "        results_data[\"is_unique\"] = results_data[\"distinct_count\"] == nrows\n",
    "        results_data[\"n_missing\"] = nrows - results_data[\"count\"]\n",
    "        results_data[\"p_missing\"] = results_data[\"n_missing\"] / nrows\n",
    "        results_data[\"p_infinite\"] = 0\n",
    "        results_data[\"n_infinite\"] = 0\n",
    "        results_data[\"range\"] = results_data[\"max\"] - results_data[\"min\"]\n",
    "        results_data[\"cv\"] = results_data[\"std\"] / results_data[\"mean\"]\n",
    "        results_data['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        results_data['p_zeros'] = results_data['n_zeros'] / nrows\n",
    "        result = results_data.iloc[:1,:]\n",
    "        result[\"memorysize\"] = 0\n",
    "        \n",
    "        #if (result[\"count\"] > result[\"distinct_count\"] > 1):\n",
    "        #    try:\n",
    "        #        result[\"mode\"] = result[\"top\"]\n",
    "        #    except KeyError:\n",
    "        #        result[\"mode\"] = 0\n",
    "        #else:\n",
    "        #    try:\n",
    "        #        result[\"mode\"] = result[\"value_counts\"].index[0]\n",
    "        #    except KeyError:\n",
    "        #        result[\"mode\"] = 0\n",
    "            \n",
    "        #    except IndexError:\n",
    "        #        result[\"mode\"] = \"MISSING\" \n",
    "                   \n",
    "        return result.iloc[:1,:].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " def describe_data(df, column, nrows):\n",
    "        column_type = df.select(column).dtypes[0][1]\n",
    "        if (\"array\" in column_type) or (\"stuct\" in column_type) or (\"map\" in column_type):\n",
    "            raise NotImplementedError(\"Column {c} is of type {t} and cannot be analyzed\".format(c=column, t=column_type))\n",
    "\n",
    "        results_data=results_data=ks.DataFrame  (\n",
    "        {'distinct_count': df.select(column).agg(countDistinct(col(column))).collect()[0][0],\n",
    "         'count': df.select(column).na.drop().select(count(col(column))).collect()[0][0],\n",
    "         'mean' : df.select(column).na.drop().agg(mean(col(column))).collect()[0][0],\n",
    "         'min': df.select(column).na.drop().agg(df_min(col(column))).collect()[0][0],\n",
    "         \"max\": df.select(column).na.drop().agg(df_max(col(column))).collect()[0][0],\n",
    "         \"variance\": df.select(column).na.drop().agg(variance(col(column))).collect()[0][0],\n",
    "         \"kurtosis\" : df.select(column).na.drop().agg(kurtosis(col(column))).collect()[0][0],\n",
    "         \"std\"    : df.select(column).na.drop().agg(stddev(col(column))).collect()[0][0],\n",
    "         \"skewness\" : df.select(column).na.drop().agg(skewness(col(column))).collect()[0][0],\n",
    "         \"sum\"     : df.select(column).na.drop().agg(df_sum(col(column))).collect()[0][0]\n",
    "        },index=[column])  \n",
    "        \n",
    "        results_data[\"p_unique\"] = results_data[\"distinct_count\"] / results_data[\"count\"]\n",
    "        results_data[\"is_unique\"] = results_data[\"distinct_count\"] == nrows\n",
    "        results_data[\"n_missing\"] = nrows - results_data[\"count\"]\n",
    "        results_data[\"p_missing\"] = results_data[\"n_missing\"] / nrows\n",
    "        results_data[\"p_infinite\"] = 0\n",
    "        results_data[\"n_infinite\"] = 0\n",
    "        results_data[\"range\"] = results_data[\"max\"] - results_data[\"min\"]\n",
    "        results_data[\"cv\"] = results_data[\"std\"] / results_data[\"mean\"]\n",
    "        results_data['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        results_data['p_zeros'] = results_data['n_zeros'] / nrows\n",
    "        #print(results_data.loc[:,1])\n",
    "        result = results_data.iloc[:1,:]\n",
    "        #print(result.head())\n",
    "        result[\"memorysize\"] = 0\n",
    "        if (result[\"count\"] > result[\"distinct_count\"] > 1):\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"top\"]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "        else:\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"value_counts\"].index[0]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "            \n",
    "            except IndexError:\n",
    "                result[\"mode\"] = \"MISSING\" \n",
    "                   \n",
    "        return result.iloc[:1,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
